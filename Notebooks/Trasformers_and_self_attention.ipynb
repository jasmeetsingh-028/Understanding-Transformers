{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaLH50y1PAFC",
        "outputId": "0008720a-6869-4559-cf44-6377df853b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 15:56:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-05-04 15:56:23 (139 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#downloading dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#opening the file and reading it\n",
        "\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "N6Ht7CqhPIlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "PCLla-ksPrxC",
        "outputId": "743ecae3-7263-46be-97b1-a8278b4b9541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])  #first 1000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yERAPOC6PdlZ",
        "outputId": "fa580a9c-4ee5-4b29-fb98-04f9cbcd66bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ugD11SPfH5",
        "outputId": "f140441d-5ccd-41be-ba9b-50dc8315ded6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMzZxeW4PlgM",
        "outputId": "d2414d03-35d2-4ac6-ab83-d1f0a4882f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'F', 'A', 'o', 'C', 'x', '$', 'M', 'g', 'U', '3', 'R', 'Z', 'c', 'I', 'E', 'm', 't', 'k', 'b', 'p', 'h', 'l', 'n', 'y', 'N', 'q', ' ', 'w', ';', 'H', '-', 'O', '!', 'a', 'B', 'T', 'G', '\\n', 'P', 'z', \"'\", '&', 'f', 'Q', ',', 'V', ':', 'J', 'd', 'j', 'K', '.', 's', 'L', 'u', 'S', 'W', 'e', 'i', 'r', '?', 'X', 'D', 'v', 'Y'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(set(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgEQ9c75QECa",
        "outputId": "3f04f981-2c3b-4b9a-9feb-66f0a7a98711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_list  = list(sorted(set(text)))"
      ],
      "metadata": {
        "id": "Udw8vLikP3CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddovP1O3QMqC",
        "outputId": "65ae33f6-b805-4e40-c592-dccde0c18be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(''.join(char_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yQyjgbjQPej",
        "outputId": "6b953379-f6e0-498a-bdf8-5e1f21cf26cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_list)"
      ],
      "metadata": {
        "id": "9cEhEvM3QbNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg3706XvQgdX",
        "outputId": "d7074117-f381-435a-8c77-7d440d7c3dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding text:"
      ],
      "metadata": {
        "id": "P7LeT-vIagWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#character level tokenizer\n",
        "\n",
        "stoi = {}\n",
        "\n",
        "for char, i in enumerate(char_list):\n",
        "  stoi[i] = char"
      ],
      "metadata": {
        "id": "6n7K5jHxSCgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj_ByKUVS2B8",
        "outputId": "6e1ae9d2-7543-4899-8863-915b239199da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def encode(text, stoi):\n",
        "  encode = []\n",
        "  for char in text:\n",
        "    encode.append(stoi[char])\n",
        "  return torch.tensor(encode, dtype = torch.long)"
      ],
      "metadata": {
        "id": "Shmy_Re7ZYYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(encode, stoi):\n",
        "  decode= []\n",
        "  for enc in encode:\n",
        "    decode.append(list(stoi.keys())[enc])\n",
        "  return (''.join(decode))"
      ],
      "metadata": {
        "id": "ri-uYy-lZk59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hii there\"\n",
        "encode_text = encode(text, stoi)\n",
        "decode_text = (decode(encode(text, stoi), stoi))"
      ],
      "metadata": {
        "id": "8kWFHB9zSQY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_text)\n",
        "print(decode_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY2sA6DYS_Me",
        "outputId": "da1df46d-7ac1-4617-98d3-e0044c437777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([46, 47, 47,  1, 58, 46, 43, 56, 43])\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other tokeninzing techniques: \n",
        "\n",
        "\n",
        "1) Google uses: sentencepiece (github) text->int, sub-word tokenizer.\n",
        "\n",
        "2) tiktoken: Open ai uses it for chat gpt."
      ],
      "metadata": {
        "id": "FMYsjJsfauvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbMr27dDkKAT",
        "outputId": "25e8b6e4-54cd-4df4-e825-7e07bbbdba0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print(f\" vocab length: {enc.n_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHRbQ-SobU0V",
        "outputId": "cdbaaa1e-1cfa-4667-ba31-6854884711f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " vocab length: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(\"hii there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQeCb5LbgbI",
        "outputId": "6cea7d8b-8000-4271-8ba4-5c3a68786ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[71, 4178, 612]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.decode(enc.encode(\"hii there\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SqXdN1Kmblvb",
        "outputId": "1392437e-a3da-4c13-9d36-df932cc4265d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hii there'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokeninzing the entire training set:"
      ],
      "metadata": {
        "id": "0sP5ddbpb_18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "text = 'hii there'\n",
        "encode_text = torch.tensor(encode(text, stoi), dtype= torch.long)\n",
        "print(f\"{encode_text, type(encode_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyG8smSPbpIM",
        "outputId": "1d7f0b78-a240-494f-ce3f-be8aa5f08ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([46, 47, 47,  1, 58, 46, 43, 56, 43]), <class 'torch.Tensor'>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-3202024718c8>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  encode_text = torch.tensor(encode(text, stoi), dtype= torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##encoding some part of dataset\n",
        "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "gQOrdwJ1cvFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode(text[:1000], stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kCfnkQJe547",
        "outputId": "43e8b98b-db83-402d-a26f-58d78e528aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
              "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
              "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
              "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
              "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
              "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
              "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
              "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
              "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
              "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
              "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
              "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
              "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
              "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
              "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
              "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
              "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
              "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
              "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
              "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
              "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
              "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
              "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
              "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
              "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
              "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
              "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
              "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
              "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
              "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
              "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
              "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
              "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
              "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
              "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
              "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
              "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
              "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
              "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
              "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
              "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
              "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
              "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(text[:1000], stoi), stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge4aJtFPfPvw",
        "outputId": "78c7536e-47cd-4bf6-e5be-d2a904683cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(text[:1000], stoi), stoi) == text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sPox8SffbEF",
        "outputId": "13554c5d-4baf-4879-ee02-21fd65196a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = encode(text, stoi)"
      ],
      "metadata": {
        "id": "OufCAem5fnU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWW7AL-Vf-Vh",
        "outputId": "ca95b45b-4a2d-4888-a155-5aa65e99b5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxvkON-Sf1Mr",
        "outputId": "b35978e7-3812-4863-ec1f-10e07b4ffc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "\n",
        "train_data = data[:int(0.9*len(data))]   #90% of the data \n",
        "val_data = data[int(0.9*len(data)):]    #rest 10% of data"
      ],
      "metadata": {
        "id": "4pTB6iDPf4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape, val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT2WMO-yhOqD",
        "outputId": "2efbfb41-9140-4044-e371-dde105e25ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854]) torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformers are based on chuks of dataset, chuncks- block size\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size+1]\n",
        "\n",
        "# 8 individual examples, in context of 18, 47 comes next\n",
        "# in context of 18,47 , 56 comes next\n",
        "\n",
        "# in context of 18,47,56   , 57 comes next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3TmmLNehZe3",
        "outputId": "f0306758-5f88-4579-d39e-aee46b99abc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time dimension\n",
        "\n",
        "X =  train_data[:block_size+1]\n",
        "for i in range(len(train_data[:block_size+1])-1):\n",
        "  print(f\"in context of {X[:i+1]} the target is {X[i+1]}\")"
      ],
      "metadata": {
        "id": "opQSvSV-iKR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a8f109-9997-47c0-c5d0-c6d75b20c369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in context of tensor([18]) the target is 47\n",
            "in context of tensor([18, 47]) the target is 56\n",
            "in context of tensor([18, 47, 56]) the target is 57\n",
            "in context of tensor([18, 47, 56, 57]) the target is 58\n",
            "in context of tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "in context of tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "in context of tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "in context of tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(train_data) - 8, (4,))"
      ],
      "metadata": {
        "id": "ahh2CLnmo15p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY3ze36zrb8E",
        "outputId": "a919d689-6c40-490a-b3c5-4065f5f725ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([722544, 978233, 781311, 386257])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "torch.stack([train_data[i:i+8] for i in ix])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqYCrJQYo_H-",
        "outputId": "b7090192-411c-4d94-c01b-322c1727086f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49, 43, 52,  0, 14, 63,  1, 39],\n",
              "        [10,  0, 25, 39, 56, 56, 63,  6],\n",
              "        [59,  6,  1, 57, 47, 56,  6,  1],\n",
              "        [41, 43,  1, 44, 43, 39, 56,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#target\n",
        "torch.stack([train_data[i+1:i+8+1] for i in ix])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROkVzAEGrRd4",
        "outputId": "732bb92e-f0d0-464b-c3f6-69e215a0be61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[43, 52,  0, 14, 63,  1, 39, 52],\n",
              "        [ 0, 25, 39, 56, 56, 63,  6,  1],\n",
              "        [ 6,  1, 57, 47, 56,  6,  1, 21],\n",
              "        [43,  1, 44, 43, 39, 56,  1, 53]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple chunks at the same time\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequence will be processed parallel to minize gpu load\n",
        "block_size = 8 # individual sequence lengths\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  rand_vals = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in rand_vals])\n",
        "  y = torch.stack([data[i+1: i+block_size+1] for i in rand_vals])\n",
        "  return x,y\n",
        "\n",
        "X,y = get_batch('train')\n",
        "print(f'inputs: {X.shape} \\n {X}')\n",
        "print(f'targets: {y.shape} \\n {y}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9GieEwrjvR9",
        "outputId": "03f046d3-2c94-4cb6-a2ba-3013d0f61e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: torch.Size([4, 8]) \n",
            " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets: torch.Size([4, 8]) \n",
            " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(batch_size): #batch_dimension\n",
        "  for t in range(block_size):  #time_dimension\n",
        "    context = X[b, :t+1]\n",
        "    target = y[b,t]\n",
        "    print(f'when input is {context} output is {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWgsdWR6sgN1",
        "outputId": "966178fc-79c1-4f29-8a9b-0efe6aa2bc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([24]) output is 43\n",
            "when input is tensor([24, 43]) output is 58\n",
            "when input is tensor([24, 43, 58]) output is 5\n",
            "when input is tensor([24, 43, 58,  5]) output is 57\n",
            "when input is tensor([24, 43, 58,  5, 57]) output is 1\n",
            "when input is tensor([24, 43, 58,  5, 57,  1]) output is 46\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46]) output is 43\n",
            "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) output is 39\n",
            "when input is tensor([44]) output is 53\n",
            "when input is tensor([44, 53]) output is 56\n",
            "when input is tensor([44, 53, 56]) output is 1\n",
            "when input is tensor([44, 53, 56,  1]) output is 58\n",
            "when input is tensor([44, 53, 56,  1, 58]) output is 46\n",
            "when input is tensor([44, 53, 56,  1, 58, 46]) output is 39\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39]) output is 58\n",
            "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) output is 1\n",
            "when input is tensor([52]) output is 58\n",
            "when input is tensor([52, 58]) output is 1\n",
            "when input is tensor([52, 58,  1]) output is 58\n",
            "when input is tensor([52, 58,  1, 58]) output is 46\n",
            "when input is tensor([52, 58,  1, 58, 46]) output is 39\n",
            "when input is tensor([52, 58,  1, 58, 46, 39]) output is 58\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58]) output is 1\n",
            "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) output is 46\n",
            "when input is tensor([25]) output is 17\n",
            "when input is tensor([25, 17]) output is 27\n",
            "when input is tensor([25, 17, 27]) output is 10\n",
            "when input is tensor([25, 17, 27, 10]) output is 0\n",
            "when input is tensor([25, 17, 27, 10,  0]) output is 21\n",
            "when input is tensor([25, 17, 27, 10,  0, 21]) output is 1\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1]) output is 54\n",
            "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) output is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJiSZ8TuvDZ",
        "outputId": "30cc6663-5ee4-4bf2-9d7d-642031b3d2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## bigram language model\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):   #bigramlangmodel is subclass of nn module\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)    #(65,65)\n",
        "\n",
        "    #so when idx is given as an input eavery interger in it ex: 24 will pluck out 24th row and 43 will pluck out 43 rd row\n",
        "\n",
        "  def forward(self, idx, target):\n",
        "\n",
        "    #idx and target are both (batch_size 'B', block_size or timen_dimension 'T') tensor of integers\n",
        "\n",
        "    logits = self.token_embeding_table(idx)  # pytorch arranges the row which were plucked out in : (B,T,C) format   \n",
        "    \n",
        "    #input idx 'X' is passed on to the token_embedding_table\n",
        "\n",
        "    return logits # scores of what comes next as we are generating things, predicting based on single current digit "
      ],
      "metadata": {
        "id": "_o9bka1HvYKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "out = m(X,y)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLYVpDU6yaHt",
        "outputId": "74226095-36e2-45bd-9707-e8832c6a05d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding loss function to it\n",
        "\n",
        "## bigram language model\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):   #bigramlangmodel is subclass of nn module\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)    #(65,65)\n",
        "\n",
        "    #so when idx is given as an input eavery interger in it ex: 24 will pluck out 24th row and 43 will pluck out 43 rd row\n",
        "\n",
        "  def forward(self, idx, target = None):\n",
        "\n",
        "    #idx and target are both (batch_size 'B', block_size or timen_dimension 'T', channels 'C' or vocab size) tensor of integers\n",
        "\n",
        "    logits = self.token_embeding_table(idx)  # pytorch arranges the row which were plucked out in : (B,T,C) format   \n",
        "    \n",
        "    #input idx 'X' is passed on to the token_embedding_table\n",
        "\n",
        "    ####loss function - negative log likelihood loss\n",
        "\n",
        "    ##cross entropy expects input in (B,C,T) format\n",
        "\n",
        "    B,T,C = logits.shape\n",
        "\n",
        "    logits = logits.view(B*T, C)   #changing dimension for loss\n",
        "    target = target.view(B*T) \n",
        "    loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, loss # scores of what comes next as we are generating things, predicting based on single current digit\n"
      ],
      "metadata": {
        "id": "_K_VutLI_-cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(X,y)\n",
        "print(out.shape, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB2CCgiu3_4Y",
        "outputId": "9f8146a6-921c-4350-c446-134a59ef21fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding generate function\n",
        "\n",
        "## adding loss function to it\n",
        "\n",
        "## bigram language model\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):   #bigramlangmodel is subclass of nn module\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)    #(65,65)\n",
        "\n",
        "    #so when idx is given as an input eavery interger in it ex: 24 will pluck out 24th row and 43 will pluck out 43 rd row\n",
        "\n",
        "  def forward(self, idx, target = None):\n",
        "\n",
        "    #idx and target are both (batch_size 'B', block_size or timen_dimension 'T', channels 'C' or vocab size) tensor of integers\n",
        "\n",
        "    logits = self.token_embeding_table(idx)  # pytorch arranges the row which were plucked out in : (B,T,C) format   \n",
        "    \n",
        "    #input idx 'X' is passed on to the token_embedding_table\n",
        "\n",
        "    ####loss function - negative log likelihood loss\n",
        "\n",
        "    ##cross entropy expects input in (B,C,T) format\n",
        "\n",
        "    if target is None:\n",
        "      loss = None\n",
        "    \n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "\n",
        "      logits = logits.view(B*T, C)   #changing dimension for loss\n",
        "      target = target.view(B*T) \n",
        "      loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, loss # scores of what comes next as we are generating things, predicting based on single current digit\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):   ##here the word history is not used just the last word is check to predict the next word but everything is beinf fed to the model\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      logits, loss = self(idx)  #calling the forward function\n",
        "      logits = logits[:, -1, :]   #logits now become (B,C), pluck out the last element from the time dimension\n",
        "      #applying softmax to get the probabilities\n",
        "\n",
        "      probs = F.softmax(logits, dim = 1) #(b,c)\n",
        "      #sample from the distribution\n",
        "\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      #append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)  #(b,t+1)\n",
        "\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "snu-Dn6zyxEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "out, loss = m(X,y)\n",
        "print(out.shape, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br9aX5W-4Q0z",
        "outputId": "5a692434-e3c0-4393-f7fe-825f9f8d6a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype = torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens = 100).tolist()[0],stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4doqAJ6AeN7",
        "outputId": "bd7d4d33-a88a-4305-a5eb-ff83e0fef52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create pytorch optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "L-K8lE4TDfpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qRoSlPKCFrSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for steps in tqdm(range(10000)):\n",
        "  #sampling a batch\n",
        "  X,y = get_batch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(X,y)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcqosQhIE6Vr",
        "outputId": "e6d5c22a-336e-4deb-bc23-d94fc6bb9865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:10<00:00, 929.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.572469472885132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx, max_new_tokens = 500).tolist()[0],stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SR-rnTxFcuh",
        "outputId": "910c9c3c-a833-4baf-f70a-465225b920f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercckehathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThineent.\n",
            "\n",
            "Lavinde.\n",
            "athave l.\n",
            "KEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenous s ls, theresseys\n",
            "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
            "I hisgothers w dere! ABer wotouciullle's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mathematical trick for self attention"
      ],
      "metadata": {
        "id": "OVSSbGpi6jMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, C = 4, 8, 2\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cif0U61VGp1h",
        "outputId": "458fc733-2e00-4673-9ca0-0e6ee20ef9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coupling tokens: token at 5th location should communicate with 5th, 4th 3rd 2nd and 1st token, this can be done by averaging of the previous tokens."
      ],
      "metadata": {
        "id": "UdQoj8AI7Suc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))"
      ],
      "metadata": {
        "id": "tat1pQuuKJT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]  #all prev instances for a particular(b,t)\n",
        "    xbow[b,t] = torch.mean(xprev, dim=0)"
      ],
      "metadata": {
        "id": "rX4xAHARKQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7_bmHEhKRDy",
        "outputId": "1771f6d9-3fec-48ac-9699-ed721d7555e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-6.6515e-01,  7.6920e-01],\n",
              "        [-1.0856e+00, -7.4231e-01],\n",
              "        [-1.1869e+00, -1.4491e+00],\n",
              "        [-8.9321e-01, -7.0390e-01],\n",
              "        [ 1.6554e-01, -1.5425e+00],\n",
              "        [-3.4596e-01, -9.2675e-02],\n",
              "        [ 7.0194e-01, -9.7682e-04],\n",
              "        [ 1.3636e-01,  6.1380e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIMhhm8S88bo",
        "outputId": "7e50348e-120a-4316-ff41-92af5fd2e7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6651,  0.7692],\n",
              "        [-0.8754,  0.0134],\n",
              "        [-0.9792, -0.4741],\n",
              "        [-0.9577, -0.5315],\n",
              "        [-0.7331, -0.7337],\n",
              "        [-0.6686, -0.6269],\n",
              "        [-0.4728, -0.5375],\n",
              "        [-0.3966, -0.3936]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x[0,0] + x[0,1])/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AwcHFG69D0R",
        "outputId": "5baaa6c8-cc1c-4b62-a834-6240daa10c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.8754,  0.0134])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37RgD0hL9L-t",
        "outputId": "7fbf7755-0a6b-47a1-e1fe-63d8c009437e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.8754,  0.0134])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x[0,0] + x[0,1] + x[0,2])/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndyYYPrU9UM9",
        "outputId": "bbdd8506-4cb3-4d8e-fcbf-08e4d2dcfb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9792, -0.4741])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0,2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpcR1V1Z9euz",
        "outputId": "38e9c319-dd17-4710-d7e0-f3546a7eecae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9792, -0.4741])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mathematical trick\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.ones(3,3)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b \n",
        "\n",
        "print(a, a.shape)\n",
        "print(b, b.shape)\n",
        "print(c, c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOC1CjY59iE3",
        "outputId": "64a77d53-967a-4115-dbc7-0435d19dbda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]) torch.Size([3, 3])\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]]) torch.Size([3, 2])\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]]) torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lower tirangular matrix\n",
        "\n",
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHFewjDh-4wV",
        "outputId": "fa8f74b3-af98-4cf3-c9fc-7ba49264fedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trick!!\n",
        "#here we are summing\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b \n",
        "\n",
        "print(a, a.shape)\n",
        "print(b, b.shape)\n",
        "print(c, c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0COu9kIz_XzA",
        "outputId": "1c23668b-767d-4c08-d5ff-d7ff643873c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [1., 1., 0.],\n",
            "        [1., 1., 1.]]) torch.Size([3, 3])\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]]) torch.Size([3, 2])\n",
            "tensor([[ 2.,  7.],\n",
            "        [ 8., 11.],\n",
            "        [14., 16.]]) torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#averaging\n",
        "#every row in a now sum upto 1 with maintaining the ower triangularity\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a/torch.sum(a, dim = 1, keepdim = True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b \n",
        "\n",
        "print(a, a.shape)\n",
        "print(b, b.shape)\n",
        "print(c, c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekP1JKdn_gA7",
        "outputId": "55c1c1c2-0781-4378-8bc9-16578911e513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]]) torch.Size([3, 3])\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]]) torch.Size([3, 2])\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]]) torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(b[0]+b[1]+b[2])/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWBX-qiJAL2a",
        "outputId": "3ebd2d33-271e-4881-8edc-221835a38c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.6667, 5.3333])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBbLoJHmAlZd",
        "outputId": "6adb350a-2ac7-4820-f47c-f49a7eb3ab58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.6667, 5.3333])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei/wei.sum(1, keepdim = True)"
      ],
      "metadata": {
        "id": "GZ47Ja_yA0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13GNlWVeB9Sy",
        "outputId": "7cdb2793-cfd9-4c3c-dec0-9be2aa9de54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = wei @ x  # (b,t,t) @ (b,t,c) --> (b,t,c)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4TYTiPlB-Tu",
        "outputId": "79f7336d-80fb-46f0-b164-90f5af27c9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk1XrDRDDNJw",
        "outputId": "46877bd4-7822-4ef3-ae66-da6c9e2fa139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6651,  0.7692],\n",
              "        [-0.8754,  0.0134],\n",
              "        [-0.9792, -0.4741],\n",
              "        [-0.9577, -0.5315],\n",
              "        [-0.7331, -0.7337],\n",
              "        [-0.6686, -0.6269],\n",
              "        [-0.4728, -0.5375],\n",
              "        [-0.3966, -0.3936]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XIghqQxXX2A",
        "outputId": "fbcf64de-1cff-41e9-a284-f22f7e249302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6651,  0.7692],\n",
              "        [-0.8754,  0.0134],\n",
              "        [-0.9792, -0.4741],\n",
              "        [-0.9577, -0.5315],\n",
              "        [-0.7331, -0.7337],\n",
              "        [-0.6686, -0.6269],\n",
              "        [-0.4728, -0.5375],\n",
              "        [-0.3966, -0.3936]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method 3\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))"
      ],
      "metadata": {
        "id": "Ku1PfVaMXnt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP-lMHAfaLTD",
        "outputId": "5fbc6de2-3c66-4206-992d-bfbad92885d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.zeros((T,T))"
      ],
      "metadata": {
        "id": "tW_wWL-maZQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJm2hnn4aezQ",
        "outputId": "2e153dcb-f0ee-48f9-a416-eb521141145c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mask filling wei by -inf where tril is 0\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))    "
      ],
      "metadata": {
        "id": "QablLIkbaf06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ8iF410a3mt",
        "outputId": "4de344c8-a58f-4412-a756-087d248d41e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying softmax to wei\n",
        "wei = F.softmax(wei, dim = 1)\n"
      ],
      "metadata": {
        "id": "YK-MYxhZa5h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei  #exact same matris as before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ZR_X9XbFwK",
        "outputId": "40c3f318-246d-47e0-e79d-55cb6349f261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow3 = wei@x"
      ],
      "metadata": {
        "id": "cG0Lh0OUbG3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow3[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E71IMGib6sJ",
        "outputId": "e4a24505-8fb6-4069-90cf-9cf9c7427d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6651,  0.7692],\n",
              "        [-0.8754,  0.0134],\n",
              "        [-0.9792, -0.4741],\n",
              "        [-0.9577, -0.5315],\n",
              "        [-0.7331, -0.7337],\n",
              "        [-0.6686, -0.6269],\n",
              "        [-0.4728, -0.5375],\n",
              "        [-0.3966, -0.3936]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmA0snJ4b-EV",
        "outputId": "11f74d38-4104-4876-fcdf-02ca8d8b32f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6651,  0.7692],\n",
              "        [-0.8754,  0.0134],\n",
              "        [-0.9792, -0.4741],\n",
              "        [-0.9577, -0.5315],\n",
              "        [-0.7331, -0.7337],\n",
              "        [-0.6686, -0.6269],\n",
              "        [-0.4728, -0.5375],\n",
              "        [-0.3966, -0.3936]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#single attention head\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)  #(b,t,16)\n",
        "q = query(x) #(b, t,16)\n",
        "wei = q @ k.transpose(-2,-1) #(b,t,16) @(b,16,t) --->(b, t, t) \n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril ==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n",
        "\n",
        "#how much information to aggregate from each tokens past\n"
      ],
      "metadata": {
        "id": "QSBrg5TXcAad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f217edef-50dc-48a7-ce65-f2ce81e677c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOvCbkG5gnXh",
        "outputId": "6542d8aa-66e2-44fc-ae41-3ef18538eb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HF5LCClqgrZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}